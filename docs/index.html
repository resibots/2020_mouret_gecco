<!doctype html>
<meta charset="utf-8">
<style>
body {
  overflow-x: hidden;
}
.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}
.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}
.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}
.spinner {
  position: absolute;
  height: 160px;
  width: 160px;
  -webkit-animation: rotation .6s infinite linear;
  -moz-animation: rotation .6s infinite linear;
  -o-animation: rotation .6s infinite linear;
  animation: rotation .6s infinite linear;
  border-left: 6px solid rgba(0, 174, 239, .15);
  border-right: 6px solid rgba(0, 174, 239, .15);
  border-bottom: 6px solid rgba(0, 174, 239, .15);
  border-top: 6px solid rgba(0, 174, 239, .8);
  border-radius: 100%;
  top: calc(50% - 100px);
  left: calc(50% - 80px);
  right: auto;
  bottom: auto;
}

@-webkit-keyframes rotation {
  from {
    -webkit-transform: rotate(0deg);
  }
  to {
    -webkit-transform: rotate(359deg);
  }
}
.transparent {
  opacity: 0;
}

figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}

.btn-group button {
  background-color: orange;
  border: 1px solid #FF6C00;
  color: white; /* White text */
  padding: 5px 12px; /* Some padding */
  cursor: pointer; /* Pointer/hand icon */
  float: center; /* Float the buttons side by side */
}

/* Add a background color on hover */
.btn-group button:hover {
  background-color: #FF6C00;
}
</style>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <!-- roboto font -->
  <link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>

  <meta name="theme-color" content="#ffffff" />


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141682504-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-141682504-1');
  </script>


  <!-- SEO -->
  <meta property="og:title" content="Weight Agnostic Neural Networks" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Networks that can already (sort of) perform tasks with random weights." />
  <meta property="og:image" content="https://weightagnostic.github.io/assets/img/wann_card_rect_v2.png" />
  <meta property="og:url" content="https://weightagnostic.github.io/" />
  <!-- Twitter Card data -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Weight Agnostic Neural Networks" />
  <meta name="twitter:description" content="" />
  <meta property="og:site_name" content="Weight Agnostic Neural Networks" />
  <meta name="twitter:image" content="https://weightagnostic.github.io/assets/img/wann_card_square_v2.png" />


</head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">

<!--<script src="lib/jquery-1.12.4.min.js"></script>
<script src="lib/mobile-detect.min.js"></script>-->
<script src="lib/template.v1.js"></script>

<script type="text/front-matter">
  title: "Automating Representation Discovery with MAP-Elites"
  description: ""
</script>
<body>
<div style="text-align: center;">
<video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/mp4/wann_cover.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="50%"><figcaption style="text-align: center;">A weight agnostic neural network performing <i>BipedalWalker-v2</i> task at various different weight parameters.</figcaption></td>
</tr></table>

</div>

<dt-article id="dtbody">

<dt-byline class="l-page transparent"></dt-byline>
<h1>Automating Representation Discovery with MAP-Elites</h1>
<p></p>
<dt-byline class="l-page" id="authors_section" hidden>
<div class="byline">
  <div class="authors">
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=GGyARB8AAAAJ&hl=en">Adam Gaier</a>
        <a class="affiliation" href="https://g.co/brain">Inria, H-BRS</a>
    </div>
    <div class="author">
        <a class="name" href="http://blog.otoro.net/">Alexander Asteroth</a>
        <a class="affiliation" href="https://g.co/brain">H-BRS</a>
    </div>
    <div class="author">
        <a class="name" href="http://blog.otoro.net/">Jean-Baptiste Mouret</a>
        <a class="affiliation" href="https://g.co/brain">Inria</a>
    </div>
  </div>
  <div class="date">
    <div class="month">June 12</div>
    <div class="year">2019</div>
  </div>
  <div class="date">
    <div class="month">Download</div>
    <div class="year" style="color: #FF6C00;"><a href="https://arxiv.org/abs/1906.04358" target="_blank">PDF</a></div>
  </div>
</div>
</dt-byline>
</dt-byline>
<h2>Abstract</h2>
<p>The way solutions are represented, or encoded, is usually the result of domain knowledge and experience. In this work, we combine MAP-Elites with Variational Autoencoders to learn a Data-Driven Encoding (DDE) that captures the essence of the highest-performing solutions while still able to encode a wide array of solutions. Our approach learns this data-driven encoding during optimization by balancing between exploiting the DDE to generalize the knowledge contained in the current archive of elites and exploring new representations that are not yet captured by the DDE. Learning representation during optimization allows the algorithm to solve high-dimensional problems, and provides a low-dimensional representation which can be then be re-used. We evaluate the DDE approach by evolving solutions for inverse kinematics of a planar arm (200 joint angles) and for gaits of a 6-legged robot in action space (a sequence of 60 positions for each of the 12 joints). We show that the DDE approach not only accelerates and improves optimization, but produces a powerful encoding that captures a bias for high performance while expressing a variety of solutions.</p>
<hr>
<h2>Introduction</h2>
<p>How solutions are represented is one of the most critical design decisions in optimization, as the representation defines the way an algorithm can move in the search space<dt-cite key="rothlauf2006representations"></dt-cite>. Work on representations tends to focus on encoding priors or innate biases: aerodynamic designs evolved with splines to encourage smooth forms <dt-cite key="olhofer2001adaptive"></dt-cite>, Compositional Pattern Producing Networks (CPPNs) introduce biases for symmetry and repetition to produce images and neural network weight patterns <dt-cite key="cppn,hyperneat"></dt-cite>, or encodings which aim to encourage modularity in neural networks<dt-cite key="mouret2008mennag,durr2010genetic,doncieux2004evolving"></dt-cite>.</p>
<p>The best representations balance a bias for high performing solutions, so they can easily be discovered, and the ability to express a diversity of potential solutions, so the the search space can be widely explored. At the one extreme, a representation which only encodes the global optimum is easy to search, but useless for finding any other solution. At the other, a representation which can encode anything presents a difficult and dauntingly vast search space.</p>
<p>Given a large set of example solutions, representations could be learned from data instead of been hand-tailored by trial-and-error: a learned representation would replicate the same biases toward performance and the same range of expressivity as the source data set. For instance, given a dataset of face images, a variational autoencoder (VAE) <dt-cite key="vae"></dt-cite> or a Generative Adversarial Network (GAN) <dt-cite key="gan"></dt-cite> can learn a low-dimensional latent space, that is, a representation, that makes it possible to explore the space of face images. In essence, the decoder which maps the latent space to the phenotypic space learns the &quot;recipe&quot; of faces. Importantly, the existence of such a low-dimensional latent space is possible because <em>the dataset is a very small part of the set of all possible images</em>.</p>
<p>However, using a dataset of preselected high-performing solutions &quot;traps&quot; the search within the distribution of solutions that are already known: a VAE trained on white faces will never generate a black face. This limits the usefulness of such data-driven representations for discovering <em>novel</em> solutions to hard problems.</p>
<p>In this paper, we propose the use of the MAP-Elites algorithm <dt-cite key="mapelites"></dt-cite> to automatically generate a dataset for representations using only a performance function and a diversity space. Quality diversity algorithms like MAP-Elites are a good fit for representation discovery: creating archives of diverse high-performing solutions is precisey their purpose. Using the MAP-Elites archive as a source of example solutions, we can capture the genetic distribution of the highest performing solutions, or elites, by training a VAE and obtaining a latent representation. As the VAE is only trained on elites, this learned representation, or data-driven encoding (DDE), has a strong bias towards solutions with high fitness; and because the elites having varying phenotypes, the DDE is able to express a range of solutions. Though the elites vary along a phenotypic continuum, they commonly have many genotypic similarities<dt-cite key="me_linemut"></dt-cite>, which makes it likely to find a good latent space.</p>
<p>Nonetheless, MAP-Elites will struggle to find high-performing solutions without an adequate representation. Fortunately, the archive is produced by MAP-Elites in an iterative, any-time fashion, so there is no &quot;end state&quot; to wait for before a DDE can be trained -- a DDE can be trained <em>during optimization</em>. The DDE can then be used to enhance the optimization process. By improving the quality of the archive, the DDE imporves the quality of its own source data, establishing a virtuous cycle of archive and encoding improvement.</p>
<p>A DDE based on an archive will encounter the same difficulty as any learned encoding: the DDE can only represent solutions that are already in the dataset. How then, can we discover new solutions? Fundamentally, to search for an encoding, we need to both <em>exploit the best known representation</em>, that is, create better solutions according to the current best &quot;recipes&quot;, and also <em>explore new representations</em> -- solutions which do not follow any &quot;recipe&quot;.</p>
<p>In this paper, we address this challenge by mixing solutions generated with the DDE with solutions obtained using standard MAP-Elites operators. Our algorithm applies classic operators, such as Gaussian mutation, to create candidates which could not be captured by the current DDE. At the same time we leverage the DDE to generalize common patterns across the map and create new solutions that are likely to be high-performing. To avoid introducing new hyper-parameters, we tune this exploration/exploitation trade-off optimally using a multi-armed bandit algorithm <dt-cite key="garivier2011upper"></dt-cite>.</p>
<p>This new algorithm, DDE-Elites, reframes optimization as a search for representations. Integrating MAP-Elites with a VAE makes it possible to apply  quality diversity to high-dimensional search spaces, and to find effective representations for future uses.</p>
<div style="text-align: center;">
<img class="b-lazy" src="assets/png/intro.png" style="width: 75%;"/>
<br/>
<figcaption style="text-align: left;">
<b>Data-Driven Encoding MAP-Elites (DDE-Elites) searches the space of representations to search for solutions</b><br/>
 A data-driven encoding (DDE) is learned by training a VAE on the MAP-Elites archive. High fitness solutions, which increase the bias of the DDE toward performance, are found using the DDE. Novel solutions, which increase the range of solutions which can be expressed, are be found using mutation operators. UCB1, a bandit algorithm, balances the mix of these explorative and exploitative operators.
</figcaption>
</div>
<p>We are interested in domains that have a straightforward low-level representation that is too high-dimensional for most algorithms, for instance:  joints positions at every time-step for a walking robot (12 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">×</span></span></span></span> 60=720 positions for a 3-second gait of a robot with 12 degrees of freedom), 3D shapes in which each voxel is encoded individually (low-level representation would be 1000-dimensional for a 10 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">×</span></span></span></span> 10 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">×</span></span></span></span> 10 grid), images encoded in the pixel-space, etc.</p>
<p>Ideally, the generated DDE will capture the main regularities of the domain. In robot locomotion, this could correspond to periodic functions, since we already know that a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mn>6</mn></mrow><annotation encoding="application/x-tex">36</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">6</span></span></span></span>-dimensional controller based on periodic functions can produce the numerous joint commands required every second to effectively drive a 12-joint walking robot in many different ways<dt-cite key=cully2015robots></dt-cite>. In many domains the space of possible solutions can be vast, while the inherent dimensionality of interesting solutions still compact. By purposefully seeking out a space of solutions, rather than individual solutions themselves, we can solve high-dimensional problems in a lower dimensional space.</p>
<hr>
<h2>Related Work</h2>
<p><em>If you would like to discuss any issues or give feedback, please visit the <a href="https://github.com/weightagnostic/weightagnostic.github.io/issues">GitHub</a> repository of this page for more information.</em></p>
</dt-article>
<dt-appendix>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">
%-------------------------------------------%
% EA Reps

  % "Perhaps the most difficult and least understood area of EA design is that of adapting its internal representation. It is clear that choices of representation play an extremely important role in the performance of an EA, but are difficult to automate"
  @incollection{de2007parameter,
    title={Parameter setting in EAs: a 30 year perspective},
    author={De Jong, Kenneth},
    booktitle={Parameter setting in evolutionary algorithms},
    pages={1--18},
    year={2007},
    publisher={Springer}
  }

  @incollection{rothlauf2006representations,
  title={Representations for genetic and evolutionary algorithms},
  author={Rothlauf, Franz},
  booktitle={Representations for Genetic and Evolutionary Algorithms},
  pages={9--32},
  year={2006},
  publisher={Springer}
  }

  @article{cppn_canalization,
    title={The emergence of canalization and evolvability in an open-ended, interactive evolutionary system},
    author={Huizinga, Joost and Stanley, Kenneth O and Clune, Jeff},
    journal={Artificial life},
    volume={24},
    number={3},
    pages={157--181},
    year={2018},
    publisher={MIT Press}
  }

  % parameter adaptation with bandits
  @inproceedings{dacosta2008adaptive,
  title={Adaptive operator selection with dynamic multi-armed bandits},
  author={DaCosta, Luis and Fialho, Alvaro and Schoenauer, Marc and Sebag, Mich{\`e}le},
  booktitle={Proceedings of the 10th annual conference on Genetic and evolutionary computation},
  pages={913--920},
  year={2008}
}

%-------------------------------------------%
% Complexifying Representations

  % Messy GAs 
  @article{goldberg1989messy,
    title={Messy genetic algorithms: Motivation, analysis, and first results},
    author={Goldberg, David E and Korb, Bradley and Deb, Kalyanmoy and others},
    journal={Complex systems},
    volume={3},
    number={5},
    pages={493--530},
    year={1989}
  }

  % Increasing genome length
  @inproceedings{altenberg1994evolving,
    title={Evolving better representations through selective genome growth},
    author={Altenberg, Lee},
    booktitle={Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence},
    pages={182--187},
    year={1994},
    organization={IEEE}
  }

  % Increasing genome / resolution of solution (airfoil)
  @inproceedings{olhofer2001adaptive,
    title={Adaptive encoding for aerodynamic shape optimization using evolution strategies},
    author={Olhofer, Markus and Jin, Yaochu and Sendhoff, Bernhard},
    booktitle={Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No. 01TH8546)},
    volume={1},
    pages={576--583},
    year={2001},
    organization={IEEE}
  }

  % Increasing genome / resolution of solution (optimal control)
  @inproceedings{gaier2014evolution,
    title={Evolution of optimal control for energy-efficient transport},
    author={Gaier, Adam and Asteroth, Alexander},
    booktitle={2014 IEEE Intelligent Vehicles Symposium Proceedings},
    pages={1121--1126},
    year={2014},
    organization={IEEE}
  }


  % Genetic Programming
  @book{koza1990genetic,
    title={Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems},
    author={Koza, John R},
    volume={34},
    year={1990},
    publisher={Stanford University, Department of Computer Science Stanford, CA}
  }

  % NEAT
  @article{neat,
  author = {Stanley, K and Miikkulainen, R},
  file = {:Users/adam/Documents/Mendeley Desktop/Stanley, Miikkulainen/2002 - Evolving neural networks through augmenting topologies.pdf:pdf},
  journal = {Evolutionary computation},
  title = {{Evolving neural networks through augmenting topologies}},
  year = {2002}
  }

  % CPPN Neat
  @article{cppn,
    title={Compositional pattern producing networks: A novel abstraction of development},
    author={Stanley, K},
    journal={Genetic programming and evolvable machines},
    year={2007},
    publisher={Springer}
  }

  % HyperNEAT
  @article{hyperneat,
  title={A hypercube-based encoding for evolving large-scale neural networks},
  author={Stanley, Kenneth O and D'Ambrosio, David B and Gauci, Jason},
  journal={Artificial life},
  volume={15},
  number={2},
  pages={185--212},
  year={2009},
  publisher={MIT Press}
  }

  % walking tables (provide oscillatory signals to controller)
  @inproceedings{clune2009evolving,
  title={Evolving coordinated quadruped gaits with the HyperNEAT generative encoding},
  author={Clune, Jeff and Beckmann, Benjamin E and Ofria, Charles and Pennock, Robert T},
  booktitle={2009 iEEE congress on evolutionary computation},
  pages={2764--2771},
  year={2009},
  organization={IEEE}
  }

  % ES-HyperNEAT
  @inproceedings{es-hyperneat,
  title={Evolving the placement and density of neurons in the hyperneat substrate},
  author={Risi, Sebastian and Lehman, Joel and Stanley, Kenneth O},
  booktitle={Proceedings of the 12th annual conference on Genetic and evolutionary computation},
  pages={563--570},
  year={2010},
  organization={ACM}
  }

  % plastic hyperneat
  @inproceedings{adaptivehyperneat,
  title={Indirectly encoding neural plasticity as a pattern of local rules},
  author={Risi, Sebastian and Stanley, Kenneth O},
  booktitle={International Conference on Simulation of Adaptive Behavior},
  pages={533--543},
  year={2010},
  organization={Springer}
}

  % Generative design
  @phdthesis{hornby2003generative,
  title={Generative Representations for Evolutionary Design Automation},
  author={Hornby, Gregory S},
  year={2003},
  school={Brandeis University}
  }

  % NAS
  @article{elsken2019neural,
    title={Neural Architecture Search: A Survey.},
    author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
    journal={Journal of Machine Learning Research},
    volume={20},
    number={55},
    pages={1--21},
    year={2019}
  }

  % NAS / CoDeepNEAT
  @incollection{miikkulainen2019evolving,
  title={Evolving deep neural networks},
  author={Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Daniel and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and others},
  booktitle={Artificial Intelligence in the Age of Neural Networks and Brain Computing},
  pages={293--312},
  year={2019},
  publisher={Elsevier}
}

%-------------------------------------------%
% Learned Representations

  % Using ANNs as geno-phenotype map
  % - Use NE to create mapping
  % - Based on benchmark functions: FFANN define reps with high locality (on average), redundancy based on neurons in input layer
  % - GP map evolved with solution genome
  % -- improved performance despite large increase in dimensionality
  @inproceedings{simoes2014self,
    title={Self-adaptive genotype-phenotype maps: neural networks as a meta-representation},
    author={Sim{\~o}es, Lu{\'\i}s F and Izzo, Dario and Haasdijk, Evert and Eiben, Agoston Endre},
    booktitle={International Conference on Parallel Problem Solving from Nature},
    pages={110--119},
    year={2014},
    organization={Springer}
  }

  % ANN-like encoding, evolved to work well on similar classes of problems
  @inproceedings{scott2015learning,
    title={Learning genetic representations for classes of real-valued optimization problems},
    author={Scott, Eric O and Bassett, Jeffrey K},
    booktitle={Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
    pages={1075--1082},
    year={2015},
    organization={ACM}
  }

  % Learned encoding (fit over many runs) overfits training landscape
  @inproceedings{scott2018toward,
  title={Toward learning neural network encodings for continuous optimization problems},
  author={Scott, Eric O and De Jong, Kenneth A},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  pages={123--124},
  year={2018},
  organization={ACM}
  }

  % Autoencoder trained on result of many runs used as representation
  @inproceedings{moreno2018learning,
    title={Learning an evolvable genotype-phenotype mapping},
    author={Moreno, Matthew Andres and Banzhaf, Wolfgang and Ofria, Charles},
    booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
    pages={983--990},
    year={2018},
    organization={ACM}
  }


  % Mario GAN
  @inproceedings{mariogan,
  title={Evolving mario levels in the latent space of a deep convolutional generative adversarial network},
  author={Volz, Vanessa and Schrum, Jacob and Liu, Jialin and Lucas, Simon M and Smith, Adam and Risi, Sebastian},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={221--228},
  year={2018},
  organization={ACM}
  }

  % Latent variable evolution 1 
  @inproceedings{latentVariableEvolution1_art,
    title={Deep interactive evolution},
    author={Bontrager, Philip and Lin, Wending and Togelius, Julian and Risi, Sebastian},
    booktitle={International Conference on Computational Intelligence in Music, Sound, Art and Design},
    pages={267--282},
    year={2018},
    organization={Springer}
  }

  % Latent variable evolution 2 
  @article{latentVariableEvolution2_fingerprint,
  title={Deepmasterprint: generating fingerprints for presentation attacks},
  author={Bontrager, Philip and Togelius, Julian and Memon, Nasir},
  journal={arXiv preprint arXiv:1705.07386},
  year={2017}
  }

  % EDAs and using VAEs go guide search
  %-------------------------------------------%

    % EDAs use probabilistic models to produce promising candidate solutions
    % - approximate the probability distribution of the fittest individuals and subsequently samples new candidate solutions from this distribution.
    @article{eda,
    title={An introduction and survey of estimation of distribution algorithms},
    author={Hauschild, Mark and Pelikan, Martin},
    journal={Swarm and evolutionary computation},
    volume={1},
    number={3},
    pages={111--128},
    year={2011},
    publisher={Elsevier}
    }

    % Denoising Autoencoder EDA
    % - denoising autoencoders as generative models for EDA to solve combinatorial optimization problems
    @inproceedings{probst2015denoising,
      title={Denoising autoencoders for fast combinatorial black box optimization},
      author={Probst, Malte},
      booktitle={Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
      pages={1459--1460},
      year={2015},
      organization={ACM}
    }    

    % VAE used to as model in EDA (model pop of promising solutions)
    @inproceedings{garciarena2018expanding,
    title={Expanding variational autoencoders for learning and exploiting latent representations in search distributions},
    author={Garciarena, Unai and Santana, Roberto and Mendiburu, Alexander},
    booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
    pages={849--856},
    year={2018},
    organization={ACM}
    }

    % Using VAE as model in EDA 
    % - built from a sliding window of historical populations
    % - fittest candidates from the sampled population history 
    % - improved sample efficiency on benchmark problems
    @inproceedings{bhattacharjee2019estimation,
    title={Estimation of Distribution using Population Queue based Variational Autoencoders},
    author={Bhattacharjee, Sourodeep and Gras, Robin},
    booktitle={2019 IEEE Congress on Evolutionary Computation (CEC)},
    pages={1406--1414},
    year={2019},
    organization={IEEE}
    }

    % Denoising AE trained on top pop from previous generation to guide search in GA
    @article{churchill2014denoising,
    title={A denoising autoencoder that guides stochastic search},
    author={Churchill, Alexander W and Sigtia, Siddharth and Fernando, Chrisantha},
    journal={arXiv preprint arXiv:1404.1614},
    year={2014}
    }




%-------------------------------------------%
% Learned Behavior Descriptors



%-------------------------------------------%
% MAP-Elites and QD

  % MAP-Elites
  @article{mapelites,
    title={Illuminating search spaces by mapping elites},
    author={Mouret, J-B and Clune, J},
    journal={arXiv preprint arXiv:1504.04909},
    year={2015}
  }

  @article{qd,
    title={Quality diversity: A new frontier for evolutionary computation},
    author={Pugh, Justin K and Soros, Lisa B and Stanley, Kenneth O},
    journal={Frontiers in Robotics and AI},
    volume={3},
    pages={40},
    year={2016},
    publisher={Frontiers}
  }

  @article{qd2,
    title={Quality and diversity optimization: A unifying modular framework},
    author={Cully, A and Demiris, Y},
    journal={IEEE Trans. on Evolutionary Computation},
    year={2018},
    publisher={IEEE}
  }

  % nature map-elites/IT&E
  @article{cully2015robots,
    title={Robots that can adapt like animals},
    author={Cully, A and Clune, J and Tarapore, D and Mouret, J-B},
    journal={Nature},
    year={2015},
    publisher={Nature Publishing Group}
  }

  % QD-picbreeder
  @inproceedings{gaier2019quality,
  title={Are quality diversity algorithms better at generating stepping stones than objective-based search?},
  author={Gaier, Adam and Asteroth, Alexander and Mouret, Jean-Baptiste},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  pages={115--116},
  year={2019}
  }

  % MAP-Elites in aerodynamics
  @inproceedings{gaier2017aerodynamic,
    title={Aerodynamic design exploration through surrogate-assisted illumination},
    author={Gaier, Adam and Asteroth, Alexander and Mouret, Jean-Baptiste},
    booktitle={18th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference},
    pages={3330},
    year={2017}
  }

  % SAIL - ECJ 
  @article{gaier2018data,
    title={Data-efficient design exploration through surrogate-assisted illumination},
    author={Gaier, Adam and Asteroth, Alexander and Mouret, Jean-Baptiste},
    journal={Evolutionary computation},
    volume={26},
    number={3},
    pages={381--410},
    year={2018},
    publisher={MIT Press}
  }


  % MAP-Elites in hearthstone - sliding boundaries
  @inproceedings{fontaine2019mapping,
  title={Mapping hearthstone deck spaces through MAP-elites with sliding boundaries},
  author={Fontaine, Matthew C and Lee, Scott and Soros, LB and De Mesentier Silva, Fernando and Togelius, Julian and Hoover, Amy K},
  booktitle={Proceedings of The Genetic and Evolutionary Computation Conference},
  pages={161--169},
  year={2019}
  }

  % cma-me
  @article{fontaine2019covariance,
  title={Covariance Matrix Adaptation for the Rapid Illumination of Behavior Space},
  author={Fontaine, Matthew C and Togelius, Julian and Nikolaidis, Stefanos and Hoover, Amy K},
  journal={arXiv preprint arXiv:1912.02400},
  year={2019}
  }

  % aurora
  @inproceedings{cully2019autonomous,
  title={Autonomous skill discovery with quality-diversity and unsupervised descriptors},
  author={Cully, Antoine},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={81--89},
  year={2019}
}

  % Line mutation
  @inproceedings{me_linemut,
    title={Discovering the elite hypervolume by leveraging interspecies correlation},
    author={Vassiliades, Vassilis and Mouret, Jean-Baptiste},
    booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
    pages={149--156},
    year={2018}
  }

  % CVT
  @article{cvt,
  title={Using centroidal voronoi tessellations to scale up the multidimensional archive of phenotypic elites algorithm},
  author={Vassiliades, Vassilis and Chatzilygeroudis, Konstantinos and Mouret, Jean-Baptiste},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={22},
  number={4},
  pages={623--630},
  year={2017},
  publisher={IEEE}
  }

%-------------------------------------------%
% MOO 

  % MOO produces variety with conflicting objectives
  @article{deb2003unveiling,
  title={Unveiling innovative design principles by means of multiple conflicting objectives},
  author={Deb, Kalyanmoy},
  journal={Engineering Optimization},
  volume={35},
  number={5},
  pages={445--470},
  year={2003},
  publisher={Taylor \& Francis}
  } 

%-------------------------------------------%
% Generative Models

  @article{ae,
    title={Reducing the dimensionality of data with neural networks},
    author={Hinton, G and Salakhutdinov, R},
    journal={Science},
    year={2006},
    publisher={American Association for the Advancement of Science}
  }

  @article{vae,
    title={Auto-encoding variational bayes},
    author={Kingma, Diederik P and Welling, Max},
    journal={arXiv preprint arXiv:1312.6114},
    year={2013}
  }

  @inproceedings{gan,
    title={Generative adversarial nets},
    author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    booktitle={Advances in neural information processing systems},
    pages={2672--2680},
    year={2014}
  }

  % KL Divergance
  @article{kldivergance,
    title={On information and sufficiency},
    author={Kullback, Solomon and Leibler, Richard A},
    journal={The annals of mathematical statistics},
    volume={22},
    number={1},
    pages={79--86},
    year={1951},
    publisher={JSTOR}
  }

  % beta-VAE 
  @article{higgins2017beta,
  title={beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  journal={International Conference on Machine Learning},
  volume={2},
  number={5},
  pages={6},
  year={2017}
  }

  % beta-VAE b
  @article{burgess2018understanding,
  title={Understanding disentangling in beta-VAE},
  author={Burgess, Christopher P and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1804.03599},
  year={2018}
  }

  % beta-TC VAE
  @inproceedings{chen2018isolating,
  title={Isolating sources of disentanglement in variational autoencoders},
  author={Chen, Tian Qi and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2610--2620},
  year={2018}
  }

  @inproceedings{kim2018disentangling,
  title={Disentangling by Factorising},
  author={Kim, Hyunjik and Mnih, Andriy},
  booktitle={International Conference on Machine Learning},
  pages={2649--2658},
  year={2018}
  }


%-------------------------------------------%
% Misc

  % NACA
  @article{naca,
    title={The characteristics of 78 related airfoil sections from tests in the variable-density wind tunnel},
    author={Jacobs, Eastman N and Ward, Kenneth E and Pinkerton, Robert M},
    year={1933}
  }

  % Splines
  @book{nurbs,
    title={The NURBS book},
    author={Piegl, Les and Tiller, Wayne},
    year={2012},
    publisher={Springer Science \& Business Media}
  }

  % UCB
  @article{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  number={2-3},
  pages={235--256},
  year={2002},
  publisher={Springer}
  }

  % Non-stationary bandit problems
  @inproceedings{garivier2011upper,
    title={On upper-confidence bound policies for switching bandit problems},
    author={Garivier, Aur{\'e}lien and Moulines, Eric},
    booktitle={International Conference on Algorithmic Learning Theory},
    pages={174--188},
    year={2011},
    organization={Springer}
  }

  % PCA
  @article{pca,
  title={Principal component analysis},
  author={Wold, Svante and Esbensen, Kim and Geladi, Paul},
  journal={Chemometrics and intelligent laboratory systems},
  volume={2},
  number={1-3},
  pages={37--52},
  year={1987},
  publisher={Elsevier}
  }

  % bayesian optimization
  @article{bo,
  title={Taking the human out of the loop: A review of Bayesian optimization},
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P and De Freitas, Nando},
  journal={Proceedings of the IEEE},
  volume={104},
  number={1},
  pages={148--175},
  year={2015},
  publisher={IEEE}
 }


%-------------------------------------------%
Modularity
@article{mouret2008mennag,
  title={MENNAG: a modular, regular and hierarchical encoding for neural-networks based on attribute grammars},
  author={Mouret, Jean-Baptiste and Doncieux, St{\'e}phane},
  journal={Evolutionary Intelligence},
  volume={1},
  number={3},
  pages={187--207},
  year={2008},
  publisher={Springer}
}

@article{durr2010genetic,
  title={Genetic representation and evolvability of modular neural controllers},
  author={Durr, Peter and Floreano, Dario and Mattiussi, Claudio},
  journal={IEEE Computational Intelligence Magazine},
  volume={5},
  number={3},
  pages={10--19},
  year={2010},
  publisher={IEEE}
}


And potentially (if there is space left for references):

@article{hornby2002creating,
  title={Creating high-level components with a generative representation for body-brain evolution},
  author={Hornby, Gregory S and Pollack, Jordan B},
  journal={Artificial life},
  volume={8},
  number={3},
  pages={223--246},
  year={2002},
  publisher={MIT Press}
}


@inproceedings{doncieux2004evolving,
  title={Evolving modular neural networks to solve challenging control problems},
  author={Doncieux, Stephane and Meyer, Jean-Arcady},
  booktitle={Proceedings of the Fourth International ICSC Symposium on engineering of intelligent systems (EIS 2004)},
  pages={1--7},
  year={2004},
  organization={ICSC Academic Press Canada}
}

%-------------------------------------------%
Misc 
% Fruit Fly Genome
@article{fruitfly,
  title={The genome sequence of Drosophila melanogaster},
  author={Adams, Mark D and Celniker, Susan E and Holt, Robert A and Evans, Cheryl A and Gocayne, Jeannine D and Amanatides, Peter G and Scherer, Steven E and Li, Peter W and Hoskins, Roger A and Galle, Richard F and others},
  journal={Science},
  volume={287},
  number={5461},
  pages={2185--2195},
  year={2000},
  publisher={American Association for the Advancement of Science}
}

% PyBullet
@article{coumans2016pybullet,
  title={Pybullet, a python module for physics simulation for games, robotics and machine learning},
  author={Coumans, Erwin and Bai, Yunfei},
  journal={GitHub repository},
  year={2016}
}

@article{cmaes,
  title={Completely derandomized self-adaptation in evolution strategies},
  author={Hansen, Nikolaus and Ostermeier, Andreas},
  journal={Evolutionary computation},
  volume={9},
  number={2},
  pages={159--195},
  year={2001},
  publisher={MIT Press}
}

@article{such2017deep,
  title={Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning},
  author={Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1712.06567},
  year={2017}
}


@article{mayne2000constrained,
  title={Constrained model predictive control: Stability and optimality},
  author={Mayne, David Q and Rawlings, James B and Rao, Christopher V and Scokaert, Pierre OM},
  journal={Automatica},
  volume={36},
  number={6},
  pages={789--814},
  year={2000},
  publisher={Elsevier}
}

@article{such2017deep,
  title={Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning},
  author={Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1712.06567},
  year={2017}
}
</script>
<!--


-->
<script language="javascript" type="text/javascript" src="lib/p5.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/p5.dom.js"></script>
<script language="javascript" type="text/javascript" src="lib/numjs.js"></script>
<script language="javascript" type="text/javascript" src="lib/agent.js"></script>
<script language="javascript" type="text/javascript" src="lib/wann_agent.js"></script>
<script language="javascript" type="text/javascript" src="lib/swingup.js"></script>
<script src="lib/blazy.js"></script>
<script language="javascript" type="text/javascript" src="lib/jquery-1.12.4.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/demo.js"></script>
<script language="javascript" type="text/javascript" src="lib/controller.js"></script>
